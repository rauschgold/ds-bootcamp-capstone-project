{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Project Costs in UK Public Sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation & Background Knowledge about Projects in UK Public Sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the Data & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is retrieved through https://www.gov.uk/government/collections/major-projects-data that collects data from 2012-2022 about the progress of projects in the Government Major Projects Portfolio. For each year we can download a `.csv` file and after downloading all of them, we compare the columns in order to merge them together. These are stored in the folder `raw_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from xgboost import XGBRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing & Merging `.csv` files into a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we found out, the column names have been changed throughout the years and in order to merge the yearly file into one in the end, we first need to ensure the correct naming. We did this actually in Excel before loading the files into the Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! We need to transpose some first and save them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Dicitionary to load and save the Dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Defining the path from which to load the .csv files\n",
    "base_path = '../data/raw_data/uk_'\n",
    "years = range(2014, 2024)\n",
    "\n",
    "# Function to load .csv files with different delimiters as we have different ones in the files\n",
    "def load_csv_with_fallback(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='iso-8859-2', delimiter=';')\n",
    "        return df\n",
    "    except pd.errors.ParserError:\n",
    "        return pd.read_csv(file_path, encoding='iso-8859-2', delimiter=',')\n",
    "\n",
    "# Loading .csv files into Dataframe and save into Dictionary\n",
    "for year in years:\n",
    "    file_path = f'{base_path}{year}.csv'\n",
    "    df_name = f'df_{year}'\n",
    "    dataframes[df_name] = load_csv_with_fallback(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the `.csv` files into separate Dataframes, we can merge them by concatinating as we also see that latest datasets have more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that merges the dataframes to one dictionary\n",
    "def merge_dataframes(dataframes_dict):\n",
    "    # Extracting all DataFrames from the dictionary and saving them as list\n",
    "    df_list = list(dataframes_dict.values())\n",
    "    \n",
    "    # Concetinating all Dataframes to one\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Calling the function to merge all Dataframes\n",
    "df = merge_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially find two columns for the colour rating but per instance only one of them contains the needed information. So we combine these into one final column for the colour rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fraus\\AppData\\Local\\Temp\\ipykernel_17112\\2742557625.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['colour_rating_new'] = df['colour_rating'].combine_first(df['colour_rating.1'])\n"
     ]
    }
   ],
   "source": [
    "df['colour_rating_new'] = df['colour_rating'].combine_first(df['colour_rating.1'])\n",
    "df = df.drop(columns=['colour_rating', 'colour_rating.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, there is some text or symbols that needs to be removed from numeric columns. We start with removing the currency symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns where the currency symbol needs to be removed\n",
    "columns_to_replace = [\n",
    "    'wlc_baseline_incl_NCG',\n",
    "    'total_baseline',\n",
    "    'forecast_incl_NGC',\n",
    "    'yearly_forecast',\n",
    "    'TOTAL Baseline Benefits (Łm)'\n",
    "]\n",
    "\n",
    "# Loop for removal of the sign\n",
    "for column in columns_to_replace:\n",
    "    df[column] = df[column].str.replace('Ł', '', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove the ',' from the numeric columns that have a 1.000-separator and make sure, the values are in float format so we can process them as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for the process\n",
    "def process_value(value):\n",
    "    # Trying to convert the value into a float\n",
    "    try:\n",
    "        float(value)\n",
    "        return value\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Second step: Replace ',' with '' and try again to convert into float\n",
    "    try:\n",
    "        value = value.replace(',', '')\n",
    "        float(value)\n",
    "        return value\n",
    "    except (ValueError, AttributeError):\n",
    "        return np.nan\n",
    "\n",
    "# Using the function to process various columns at once\n",
    "columns_to_process = ['total_baseline', 'forecast_incl_NGC', 'wlc_baseline_incl_NCG', 'TOTAL Baseline Benefits (Łm)', 'yearly_forecast']\n",
    "df[columns_to_process] = df[columns_to_process].applymap(process_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Imputing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing dates, missing benefits etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview of basic statistical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "\n",
    "- Drop negative Budget? What does that mean?\n",
    "\n",
    "- neg. benefits?\n",
    "\n",
    "- neg. year duration\n",
    "\n",
    "- year frame 1987-2075?\n",
    "\n",
    "- end date 2011?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Overview\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot view for Presentation\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "missing_values_filtered = missing_values[missing_values > 200]\n",
    "\n",
    "missing_values_filtered.to_csv('missing_values_filtered.csv', index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_values_filtered.plot(kind='bar', color='#ffed00')\n",
    "plt.title('Overview Missing Values')\n",
    "plt.xlabel('Column Name')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Plot to show Development of Number of Projects per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_per_year = df.groupby('year')['project_name'].count()\n",
    "\n",
    "projects_per_year.to_csv('projects_per_year.csv', index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "projects_per_year.plot(kind='bar', color='#ffed00')\n",
    "plt.title('Number of Projects per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Projects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Plot to show Development of Number of Projects per Year and Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_per_year_category = df.groupby(['year', 'report_category'])['project_name'].count().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "projects_per_year_category.plot(kind='bar', stacked=False, color=plt.cm.Paired.colors, figsize=(12, 8))\n",
    "\n",
    "projects_per_year_category.to_csv('projects_per_year_category.csv', index=True)\n",
    "\n",
    "\n",
    "plt.title('Number of Projects per Category and Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Projects')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Cost Allocation as Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df['wlc_baseline_incl_NCG'].dropna())\n",
    "\n",
    "plt.ylim(0, df['wlc_baseline_incl_NCG'].quantile(0.90))\n",
    "\n",
    "plt.title('Boxplot of Whole Life Costs - cut at 90% quantile')\n",
    "plt.ylabel('Whole Life Costs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "\n",
    "plt.title('Correlationmatrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Preps (needs to be updated with final df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataframe from saved `.pkl`-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/pickle/final_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the DataFrame by 'project_name' and 'year'\n",
    "df.sort_values(by=['project_name', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Forecast Variance (Yearly Forecast - Previous Yearly Forecast)\n",
    "df['forecast_variance_prev_year'] = df.groupby('project_name')['yearly_forecast'].pct_change() * 100\n",
    "\n",
    "# Calculating Budget Variance (Yearly Forecast - Yearly Budget)\n",
    "df['forecast_variance_budget'] = ((df['yearly_forecast'] - df['yearly_budget']) / df['yearly_budget']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage change trend for each project\n",
    "df['forecast_percentage_change_prev_year_filled'] = df.groupby('project_name')['forecast_variance_prev_year'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast_variance_prev_year'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Last Observation Carried Forward (LOCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCF forecast MSE: 495769.72430543805\n"
     ]
    }
   ],
   "source": [
    "# LOCF baseline-model for costs (Forecast) based on £\n",
    "# Approach: We are taking the last two instances\n",
    "\n",
    "# Creating the predicted values based on the LOCF-method\n",
    "df['forecast_pred'] = df['yearly_forecast'].shift(1)\n",
    "\n",
    "# There is no previous value for the first entry, so the predicted value remains NaN\n",
    "# The model ignores the first entry as it has no previous point\n",
    "\n",
    "# Calculating the errors for the baseline model\n",
    "mse_forecast = mean_squared_error(df['yearly_forecast'].iloc[1:], df['forecast_pred'].iloc[1:])\n",
    "\n",
    "print(f\"LOCF forecast MSE: {mse_forecast}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCF forecast MSE: 1084528.7616024816\n"
     ]
    }
   ],
   "source": [
    "# LOCF baseline-model for costs (Forecast) based on %\n",
    "# Approach: We are taking the last two instances\n",
    "\n",
    "# Creating the predicted values based on the LOCF-method\n",
    "df['forecast_pred_percentage'] = df['forecast_variance_prev_year'].shift(1)\n",
    "\n",
    "# There is no previous value for the first entry, so the predicted value remains NaN\n",
    "# The model ignores the first entry as it has no previous point\n",
    "\n",
    "df['forecast_pred_percentage'].fillna(0, inplace=True)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Calculating the errors for the baseline model\n",
    "mse_forecast = mean_squared_error(df['forecast_variance_prev_year'].iloc[1:], df['forecast_pred_percentage'].iloc[1:])\n",
    "\n",
    "print(f\"LOCF forecast MSE: {mse_forecast}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Calculating Forecast based on Variance Percentage Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the last known forecast value and the trend of the percentage change\n",
    "last_forecast = df.groupby('project_name').last()['yearly_forecast']\n",
    "last_percentage_change = df.groupby('project_name').last()['forecast_percentage_change_prev_year_filled']\n",
    "\n",
    "# Calculating forecast for 2024\n",
    "predicted_forecast_2024 = last_forecast * (1 + last_percentage_change / 100)\n",
    "\n",
    "# Creating new DataFrame including 2024 data\n",
    "df_2024 = pd.DataFrame({\n",
    "    'project_name': last_forecast.index,\n",
    "    'year': 2024,\n",
    "    'predicted_forecast': predicted_forecast_2024\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy/MSE calculate based on past years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Mean/Median Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          project_name  year   \n",
      "0                      10,000 Additional Prison Places  2024  \\\n",
      "1    10,000 Additional Prison Places Programme - Es...  2024   \n",
      "2    10K Additional Prison Places Estate Expansion ...  2024   \n",
      "3    10K additional Prison places Women's Estate Ex...  2024   \n",
      "4             10k Additional Prison Places - New Build  2024   \n",
      "..                                                 ...   ...   \n",
      "335                Workplace and Facilities Management  2024   \n",
      "336                    YOI Education Services Retender  2024   \n",
      "337                     YOUTH JUSTICE REFORM PROGRAMME  2024   \n",
      "338                              Youth Investment Fund  2024   \n",
      "339                     Youth Justice Reform Programme  2024   \n",
      "\n",
      "     predicted_forecast_mean  predicted_forecast_median  \n",
      "0                  80.923483                        0.0  \n",
      "1                  80.923483                        0.0  \n",
      "2                  80.923483                        0.0  \n",
      "3                  80.923483                        0.0  \n",
      "4                  80.923483                        0.0  \n",
      "..                       ...                        ...  \n",
      "335                80.923483                        0.0  \n",
      "336                80.923483                        0.0  \n",
      "337                80.923483                        0.0  \n",
      "338                80.923483                        0.0  \n",
      "339                80.923483                        0.0  \n",
      "\n",
      "[340 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculating mean of 'forecast_variance_prev_year'\n",
    "mean_forecast = df['forecast_variance_prev_year'].mean()\n",
    "\n",
    "# Berechne den Median der Spalte 'yearly_forecast'\n",
    "median_forecast = df['forecast_variance_prev_year'].median()\n",
    "\n",
    "# Erstelle Vorhersagen für ein neues Jahr (z.B. 2024) basierend auf dem Mean und Median\n",
    "df['mean_forecast_prediction'] = mean_forecast\n",
    "df['median_forecast_prediction'] = median_forecast\n",
    "\n",
    "# Optional: Ergebnisse für das nächste Jahr in einem neuen DataFrame\n",
    "df_2024_1 = pd.DataFrame({\n",
    "    'project_name': df['project_name'].unique(),\n",
    "    'year': 2024,\n",
    "    'predicted_forecast_mean': mean_forecast,\n",
    "    'predicted_forecast_median': median_forecast\n",
    "})\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(df_2024_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test data\n",
    "y_train = \n",
    "y_test = \n",
    "\n",
    "# Mean Predictor\n",
    "mean_pred = np.mean(y_train)\n",
    "y_pred_mean = np.full(y_test.shape, mean_pred)\n",
    "\n",
    "# Median Predictor (optional)\n",
    "median_pred = np.median(y_train)\n",
    "y_pred_median = np.full(y_test.shape, median_pred)\n",
    "\n",
    "# Berechnung der Fehlermetrik (z.B. Mean Squared Error)\n",
    "mse_mean = mean_squared_error(y_test, y_pred_mean)\n",
    "mse_median = mean_squared_error(y_test, y_pred_median)\n",
    "\n",
    "print(f\"Mean Predictor MSE: {mse_mean}\")\n",
    "print(f\"Median Predictor MSE: {mse_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duration, dummies, NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling, pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model 1: CATBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inital Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['wlc_baseline_incl_NCG'])\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best Params: {best_params}')\n",
    "print(f'Best negative MSE (Training): {best_score}')\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (Test): {mse}')\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing with Bayesian Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['wlc_baseline_incl_NCG'])\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def xgb_eval(max_depth, learning_rate, subsample, colsample_bytree):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'n_estimators': 100,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    return score\n",
    "\n",
    "# Define the search space\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer = BayesianOptimization(f=xgb_eval, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(f\"Best hyperparameters: {optimizer.max['params']}\")\n",
    "print(f\"Best score: {optimizer.max['target']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model 3: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['wlc_baseline_incl_NCG'])\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})\n",
    "print(coefficients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
