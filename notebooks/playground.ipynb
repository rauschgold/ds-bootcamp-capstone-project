{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just to hide things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_values = ['Amber', 'Green', 'Red', 'Amber/red', 'Amber/Green']\n",
    "result['colour_rating_new'] = result['colour_rating_new'].apply(lambda x: x if x in allowed_values else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.rename(columns={'colour_rating_new': 'colour_rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['yearly_forecast_new'] = result['yearly_forecast'].combine_first(result['forecast_incl_NGC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop(columns=['yearly_forecast', 'forecast_incl_NGC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.rename(columns={'yearly_forecast_new': 'yearly_forecast'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop(columns=['yearly_budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.rename(columns={'total_baseline': 'yearly_budget'})\n",
    "result = result.rename(columns={'TOTAL Baseline Benefits (Łm)': 'total_benefits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['project_name','department','colour_rating','description_aims','rating_comment','start_date','end_date','schedule_comment','yearly_budget','yearly_forecast','wlc_baseline_incl_NCG','variance_comment','budget_comment','year','report_category','project_number','total_benefits','benefits_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_macro = pd.read_csv('../data/macro/GDP.csv',encoding='iso-8859-2', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_macro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Identifizieren der relevanten Zeilen\n",
    "remained_scheduled_df = df_franzi[df_franzi['schedule_comment'].str.contains('remained scheduled', case=False)]\n",
    "\n",
    "# Schritt 2: Aktuellsten 'end_date'-Wert für jedes 'project_name' ermitteln\n",
    "latest_end_dates = df_franzi.groupby('project_name').apply(lambda x: x.loc[x['year'].idxmax(), 'end_date'])\n",
    "\n",
    "# Schritt 3: Zuweisen des aktuellsten 'end_date' an die relevanten Zeilen\n",
    "for index, row in remained_scheduled_df.iterrows():\n",
    "    latest_end_date = latest_end_dates[row['project_name']]\n",
    "    if pd.isna(row['end_date']) and pd.notna(latest_end_date):\n",
    "        df_franzi.at[index, 'end_date'] = latest_end_date\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "df_franzi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Aktuellsten 'end_date'-Wert für jedes 'project_name' ermitteln\n",
    "latest_end_dates = df_franzi.groupby('project_name').apply(lambda x: x.loc[x['year'].idxmax(), 'start_date'])\n",
    "\n",
    "# Schritt 3: Zuweisen des aktuellsten 'end_date' an die relevanten Zeilen\n",
    "for index, row in remained_scheduled_df.iterrows():\n",
    "    latest_end_date = latest_end_dates[row['project_name']]\n",
    "    if pd.isna(row['start_date']) and pd.notna(latest_end_date):\n",
    "        df_franzi.at[index, 'start_date'] = latest_end_date\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "df_franzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    month_mapping = {\n",
    "        'jan': '01', 'january': '01',\n",
    "        'feb': '02', 'february': '02',\n",
    "        'mar': '03', 'march': '03',\n",
    "        'apr': '04', 'april': '04',\n",
    "        'may': '05', 'may': '05',\n",
    "        'jun': '06', 'june': '06',\n",
    "        'jul': '07', 'july': '07',\n",
    "        'aug': '08', 'august': '08',\n",
    "        'sep': '09', 'september': '09',\n",
    "        'oct': '10', 'october': '10',\n",
    "        'nov': '11', 'november': '11',\n",
    "        'dec': '12', 'december': '12'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_after_to_or_on(comment):\n",
    "    REGEX = \"(to|is|on)\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{2,4})\"\n",
    "    x = re.compile(REGEX)\n",
    "    \n",
    "    match_REGEX = x.search(comment)      \n",
    "    \n",
    "    if match_REGEX:\n",
    "        day = match_REGEX.group(2)\n",
    "        month = match_REGEX.group(3)\n",
    "        year = match_REGEX.group(4)\n",
    "        if len(day) == 1:\n",
    "            day = '0'+ day\n",
    "        month = month_mapping.get(month.lower())\n",
    "        if len(year) == 2:\n",
    "            year = '20' + year\n",
    "        return year +'-'+ month +'-'+ day\n",
    "    \n",
    "for index, row in df_franzi.iterrows():\n",
    "    if pd.isna(row['end_date']):\n",
    "        extracted_date = extract_date_after_to_or_on(row['schedule_comment'])\n",
    "        if extracted_date:\n",
    "            df_franzi.at[index, 'end_date'] = extracted_date\n",
    "\n",
    "df_franzi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Code zum Filtern von Zeilen, in denen 'start_date' oder 'end_date' 'NaN' ist\n",
    "# und die 'schedule_comment' nicht mit 'Exempt' oder '\\r\\nExempt' beginnen\n",
    "\n",
    "nan_dates_df = df_franzi[\n",
    "    ((df_franzi['end_date'].isna())) & \n",
    "    (~df_franzi['schedule_comment'].str.match(r'^(Exempt|\\r\\nExempt)'))\n",
    "][['start_date', 'end_date', 'schedule_comment']]\n",
    "\n",
    "# Ausgabe des gefilterten DataFrames\n",
    "nan_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Code zum Filtern von Zeilen, in denen 'start_date' oder 'end_date' 'NaN' ist\n",
    "# und die 'schedule_comment' nicht mit 'Exempt' oder '\\r\\nExempt' beginnen\n",
    "\n",
    "nan_dates_df_start = df_franzi[\n",
    "    ((df_franzi['start_date'].isna())) & \n",
    "    (~df_franzi['schedule_comment'].str.match(r'^(Exempt|\\r\\nExempt)'))\n",
    "][['start_date', 'schedule_comment']]\n",
    "\n",
    "# Ausgabe des gefilterten DataFrames\n",
    "nan_dates_df_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(\"../data/pickle/cleaned_2021_2023.pkl\")\n",
    "df2 = pd.read_pickle('../data/pickle/cleaned_2018_2020.pkl')\n",
    "df3 = pd.read_pickle('../data/pickle/cleaned_2014-2017.pkl')\n",
    "df4 = pd.read_csv('../data/macro/GDP.csv',encoding='iso-8859-2', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df4, on='year', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colour_rating\"] = df[\"colour_rating\"].str.lower()\n",
    "\n",
    "# Drop rows where 'project_name' contains 'Unnamed:' followed by other characters\n",
    "df = df[~df['project_name'].str.contains(r'Unnamed:.*', na=False)]\n",
    "\n",
    "# Optionally, reset the index of the cleaned DataFrame\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_start_dates = df.groupby('project_name').apply(lambda x: x.loc[x['year'].idxmax(), 'start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    latest_start_date = latest_start_dates[row['project_name']]\n",
    "    if pd.isna(row['start_date']) and pd.notna(latest_start_date):\n",
    "        df.at[index, 'start_date'] = latest_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_benefits = df.groupby('project_name').apply(lambda x: x.loc[x['year'].idxmax(), 'total_benefits'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    latest_benefit = latest_benefits[row['project_name']]\n",
    "    if pd.isna(row['total_benefits']) and pd.notna(latest_benefit):\n",
    "        df.at[index, 'total_benefits'] = latest_benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_categories = df.groupby('project_name').apply(lambda x: x.loc[x['year'].idxmax(), 'report_category'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    latest_category = latest_categories[row['project_name']]\n",
    "    if pd.isna(row['report_category']) and pd.notna(latest_category):\n",
    "        df.at[index, 'report_category'] = latest_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_numbers = df.groupby('project_name').apply(lambda x: x.loc[x['year'].idxmax(), 'project_number'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    latest_number = latest_numbers[row['project_name']]\n",
    "    if pd.isna(row['project_number']) and pd.notna(latest_number):\n",
    "        df.at[index, 'project_number'] = latest_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description_aims'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies = pd.get_dummies(df, columns=['department', 'colour_rating'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"])\n",
    "df[\"end_date\"] = pd.to_datetime(df[\"end_date\"])\n",
    "# Calculate the duration in years\n",
    "df[\"year_duration\"] = (df[\"end_date\"] - df[\"start_date\"]).dt.days / 365.25\n",
    "\n",
    "# Optionally, round the duration to a specific number of decimal places\n",
    "df[\"year_duration\"] = df[\"year_duration\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_year'] = df['start_date'].dt.year\n",
    "df['end_year'] = df['end_date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add macro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_columns = pd.read_csv('../data/macro/GDP.csv', encoding='iso-8859-2', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['GDP', 'annual_earning_ft', 'unemployment_rate', 'youth_unemployment_rate', 'inflation_rate', 'population', 'gov_debt', 'tax_revenue', 'revenue_excl_grants', 'grants_and_other_revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(macro_columns, on='year', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/pickle/final_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['description_aims', 'rating_comment', 'start_date',\n",
    "       'end_date', 'schedule_comment', 'yearly_budget', 'yearly_forecast', 'variance_comment', 'budget_comment',\n",
    "       'report_category', 'project_number', 'total_benefits',\n",
    "       'benefits_comment', 'population', 'revenue_excl_grants', 'grants_and_other_revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/pickle/final_wo_errors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colour_rating_columns = [col for col in df.columns if col.startswith('colour_rating_')]\n",
    "# department_columns = [col for col in df.columns if col.startswith('department_')]\n",
    "\n",
    "# def latest_value(df, column):\n",
    "#     try:\n",
    "#         return df.loc[df['year'].idxmax(), column]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in Group {df.name} for column {column}: {e}\")\n",
    "#         return None\n",
    "\n",
    "agg_dict = {\n",
    "    'wlc_baseline_incl_NCG': 'max',\n",
    "    'year_duration': 'max',\n",
    "    'end_year': 'max',\n",
    "    'start_year': 'min',\n",
    "    'year': 'max',\n",
    "    'GDP': 'mean',\n",
    "    'annual_earning_ft': 'mean',\n",
    "    'unemployment_rate': 'mean',\n",
    "    'youth_unemployment_rate': 'mean',\n",
    "    'inflation_rate': 'mean',\n",
    "    'gov_debt': 'mean',\n",
    "    'tax_revenue': 'mean',\n",
    "    'department_CO': lambda x: x.mode()[0],\n",
    "    'department_CPS': lambda x: x.mode()[0],\n",
    "    'department_DBT': lambda x: x.mode()[0],\n",
    "    'department_DCMS': lambda x: x.mode()[0],\n",
    "    'department_DEFRA': lambda x: x.mode()[0],\n",
    "    'department_DEFRA & DFT': lambda x: x.mode()[0],\n",
    "    'department_DESNZ': lambda x: x.mode()[0],\n",
    "    'department_DFE': lambda x: x.mode()[0],\n",
    "    'department_DFID': lambda x: x.mode()[0],\n",
    "    'department_DFT': lambda x: x.mode()[0],\n",
    "    'department_DHSC': lambda x: x.mode()[0],\n",
    "    'department_DLUHC': lambda x: x.mode()[0],\n",
    "    'department_DSIT': lambda x: x.mode()[0],\n",
    "    'department_DWP': lambda x: x.mode()[0],\n",
    "    'department_FCDO': lambda x: x.mode()[0],\n",
    "    'department_HMLR': lambda x: x.mode()[0],\n",
    "    'department_HMRC': lambda x: x.mode()[0],\n",
    "    'department_HMT': lambda x: x.mode()[0],\n",
    "    'department_HO': lambda x: x.mode()[0],\n",
    "    'department_MOD': lambda x: x.mode()[0],\n",
    "    'department_MOJ': lambda x: x.mode()[0],\n",
    "    'department_NCA': lambda x: x.mode()[0],\n",
    "    'department_NS&I': lambda x: x.mode()[0],\n",
    "    'department_ONS': lambda x: x.mode()[0],\n",
    "    'department_VOA': lambda x: x.mode()[0],\n",
    "    'colour_rating_amber/green': lambda x: x.mode()[0],\n",
    "    'colour_rating_amber/red': lambda x: x.mode()[0],\n",
    "    'colour_rating_green': lambda x: x.mode()[0],\n",
    "    'colour_rating_red': lambda x: x.mode()[0],\n",
    "    'colour_rating_reset': lambda x: x.mode()[0]\n",
    "}\n",
    "\n",
    "# for col in colour_rating_columns:\n",
    "#     agg_dict[col] = lambda x, col=col: latest_value(x, col)\n",
    "\n",
    "# for col in department_columns:\n",
    "#     agg_dict[col] = lambda x, col=col: latest_value(x, col)\n",
    "\n",
    "df = df.groupby('project_name').agg(agg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['revenue_excl_grants','grants_and_other_revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['end_year', 'youth_unemployment_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df.drop(columns=['wlc_baseline_incl_NCG'])\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})\n",
    "print(coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "X = df.drop(columns=['wlc_baseline_incl_NCG'])\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best Params: {best_params}')\n",
    "print(f'Best negative MSE (Training): {best_score}')\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (Test): {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year_duration                98\n",
      "start_year                   77\n",
      "GDP                           0\n",
      "annual_earning_ft             0\n",
      "unemployment_rate             0\n",
      "inflation_rate                0\n",
      "gov_debt                      0\n",
      "tax_revenue                   0\n",
      "department_CO                 0\n",
      "department_CPS                0\n",
      "department_DBT                0\n",
      "department_DCMS               0\n",
      "department_DEFRA              0\n",
      "department_DEFRA & DFT        0\n",
      "department_DESNZ              0\n",
      "department_DFE                0\n",
      "department_DFID               0\n",
      "department_DFT                0\n",
      "department_DHSC               0\n",
      "department_DLUHC              0\n",
      "department_DSIT               0\n",
      "department_DWP                0\n",
      "department_FCDO               0\n",
      "department_HMLR               0\n",
      "department_HMRC               0\n",
      "department_HMT                0\n",
      "department_HO                 0\n",
      "department_MOD                0\n",
      "department_MOJ                0\n",
      "department_NCA                0\n",
      "department_NS&I               0\n",
      "department_ONS                0\n",
      "department_VOA                0\n",
      "colour_rating_amber/green     0\n",
      "colour_rating_amber/red       0\n",
      "colour_rating_green           0\n",
      "colour_rating_red             0\n",
      "colour_rating_reset           0\n",
      "dtype: int64\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['wlc_baseline_incl_NCG'])\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "print(X.isna().sum())\n",
    "print(y.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fillna(X.median(), inplace=True)\n",
    "y.fillna(y.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | subsample |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.1237   \u001b[39m | \u001b[39m0.6873   \u001b[39m | \u001b[39m0.2857   \u001b[39m | \u001b[39m8.124    \u001b[39m | \u001b[39m0.7993   \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.2275   \u001b[39m | \u001b[35m0.578    \u001b[39m | \u001b[35m0.05524  \u001b[39m | \u001b[35m3.407    \u001b[39m | \u001b[35m0.9331   \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.2289   \u001b[39m | \u001b[35m0.8006   \u001b[39m | \u001b[35m0.2153   \u001b[39m | \u001b[35m3.144    \u001b[39m | \u001b[35m0.985    \u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.2475   \u001b[39m | \u001b[35m0.9162   \u001b[39m | \u001b[35m0.07158  \u001b[39m | \u001b[35m4.273    \u001b[39m | \u001b[35m0.5917   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.19     \u001b[39m | \u001b[39m0.6521   \u001b[39m | \u001b[39m0.1622   \u001b[39m | \u001b[39m6.024    \u001b[39m | \u001b[39m0.6456   \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.1348   \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m4.534    \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.1107   \u001b[39m | \u001b[39m0.8537   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.087    \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.09153  \u001b[39m | \u001b[39m0.9827   \u001b[39m | \u001b[39m0.1481   \u001b[39m | \u001b[39m7.796    \u001b[39m | \u001b[39m0.9787   \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.1274   \u001b[39m | \u001b[39m0.7604   \u001b[39m | \u001b[39m0.1703   \u001b[39m | \u001b[39m9.984    \u001b[39m | \u001b[39m0.6671   \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.2088   \u001b[39m | \u001b[39m0.7845   \u001b[39m | \u001b[39m0.1152   \u001b[39m | \u001b[39m6.175    \u001b[39m | \u001b[39m0.513    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.1332   \u001b[39m | \u001b[39m0.9685   \u001b[39m | \u001b[39m0.2277   \u001b[39m | \u001b[39m5.028    \u001b[39m | \u001b[39m0.9349   \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.1493   \u001b[39m | \u001b[39m0.6247   \u001b[39m | \u001b[39m0.245    \u001b[39m | \u001b[39m5.574    \u001b[39m | \u001b[39m0.9532   \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.2034   \u001b[39m | \u001b[39m0.5679   \u001b[39m | \u001b[39m0.03551  \u001b[39m | \u001b[39m6.723    \u001b[39m | \u001b[39m0.5554   \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.2414   \u001b[39m | \u001b[39m0.9607   \u001b[39m | \u001b[39m0.06627  \u001b[39m | \u001b[39m4.024    \u001b[39m | \u001b[39m0.6868   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.189    \u001b[39m | \u001b[39m0.7024   \u001b[39m | \u001b[39m0.05525  \u001b[39m | \u001b[39m8.665    \u001b[39m | \u001b[39m0.5915   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.2145   \u001b[39m | \u001b[39m0.9904   \u001b[39m | \u001b[39m0.1488   \u001b[39m | \u001b[39m4.09     \u001b[39m | \u001b[39m0.5803   \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.243    \u001b[39m | \u001b[39m0.9519   \u001b[39m | \u001b[39m0.09309  \u001b[39m | \u001b[39m4.204    \u001b[39m | \u001b[39m0.6334   \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.2391   \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.05306  \u001b[39m | \u001b[39m4.339    \u001b[39m | \u001b[39m0.6869   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.08668  \u001b[39m | \u001b[39m0.5495   \u001b[39m | \u001b[39m0.2448   \u001b[39m | \u001b[39m8.628    \u001b[39m | \u001b[39m0.9876   \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.2414   \u001b[39m | \u001b[39m0.9215   \u001b[39m | \u001b[39m0.07689  \u001b[39m | \u001b[39m4.278    \u001b[39m | \u001b[39m0.597    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.1218   \u001b[39m | \u001b[39m0.8855   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.243    \u001b[39m | \u001b[39m0.6946   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.2334   \u001b[39m | \u001b[39m0.9977   \u001b[39m | \u001b[39m0.09084  \u001b[39m | \u001b[39m4.111    \u001b[39m | \u001b[39m0.6751   \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.1321   \u001b[39m | \u001b[39m0.9517   \u001b[39m | \u001b[39m0.2107   \u001b[39m | \u001b[39m6.284    \u001b[39m | \u001b[39m0.5605   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.1753   \u001b[39m | \u001b[39m0.6516   \u001b[39m | \u001b[39m0.1759   \u001b[39m | \u001b[39m6.009    \u001b[39m | \u001b[39m0.6596   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.2436   \u001b[39m | \u001b[39m0.9911   \u001b[39m | \u001b[39m0.05582  \u001b[39m | \u001b[39m4.235    \u001b[39m | \u001b[39m0.5533   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.1596   \u001b[39m | \u001b[39m0.5123   \u001b[39m | \u001b[39m0.1488   \u001b[39m | \u001b[39m8.295    \u001b[39m | \u001b[39m0.7631   \u001b[39m |\n",
      "| \u001b[35m27       \u001b[39m | \u001b[35m0.2483   \u001b[39m | \u001b[35m0.9585   \u001b[39m | \u001b[35m0.058    \u001b[39m | \u001b[35m4.029    \u001b[39m | \u001b[35m0.6664   \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.2324   \u001b[39m | \u001b[39m0.9115   \u001b[39m | \u001b[39m0.1346   \u001b[39m | \u001b[39m4.236    \u001b[39m | \u001b[39m0.5261   \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.1965   \u001b[39m | \u001b[39m0.9166   \u001b[39m | \u001b[39m0.02831  \u001b[39m | \u001b[39m4.326    \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[35m30       \u001b[39m | \u001b[35m0.2521   \u001b[39m | \u001b[35m0.6195   \u001b[39m | \u001b[35m0.2977   \u001b[39m | \u001b[35m3.869    \u001b[39m | \u001b[35m0.6585   \u001b[39m |\n",
      "=========================================================================\n",
      "Best hyperparameters: {'colsample_bytree': 0.6195386378695255, 'learning_rate': 0.2976885500743844, 'max_depth': 3.8688489104270833, 'subsample': 0.6584796522070571}\n",
      "Best score: 0.2521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X = df[['year_duration'] + [col for col in df.columns if 'department_' in col]]\n",
    "y = df['wlc_baseline_incl_NCG']\n",
    "\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def xgb_eval(max_depth, learning_rate, subsample, colsample_bytree):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'n_estimators': 100,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    return score\n",
    "\n",
    "# Define the search space\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer = BayesianOptimization(f=xgb_eval, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "best_params = optimizer.max['params']\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(f\"Best hyperparameters: {optimizer.max['params']}\")\n",
    "print(f\"Best score: {optimizer.max['target']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.6195386378695255, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.2976885500743844, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.6195386378695255, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.2976885500743844, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.6195386378695255, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.2976885500743844, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Round best_params 'max_depth' result by saving as integer\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "\n",
    "# Initialising model with best parameters\n",
    "best_model = XGBRegressor(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    n_estimators=100,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with best parameters\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Test): 64672482.55241777\n",
      "Root Mean Squared Error (Test): 8041.920327410473\n",
      "Mean Absolute Percentage Error (MAPE): inf%\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_true = y_test\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (Test): {mse}')\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Root Mean Squared Error (Test): {rmse}')\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
