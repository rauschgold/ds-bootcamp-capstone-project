{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/ Mean Absolute Error (MAE):\n",
    "mae = mean_absolute_error(y_test, y_pred).round(2)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "#2/ Mean Squared Error (MSE):\n",
    "mse = mean_squared_error(y_test, y_pred).round(2)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "# 3/ Root Mean Squared Error (RMSE):\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False).round(2)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "#4/ R-squared (Coefficient of Determination):\n",
    "# Measures how well the variance in the target variable is explained by the model.\n",
    "# Ranges from 0 to 1; a higher value indicates better performance.\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred).round(2)\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "\n",
    "# 5/ Mean Absolute Percentage Error (MAPE):\n",
    "#Measures the average percentage error between true and predicted values, which helps in understanding relative error. \n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape * 100}%')\n",
    "print('mape before percentage:', mape)# out of curiosity I wanted to see the number without e+17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1/ Feature Importance:\n",
    "# Analyzing feature importance helps in understanding which features the model relies on most. CatBoost provides built-in methods to visualize feature importance.\n",
    "model.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2/ Residual Analysis:\n",
    "#Plot residuals (errors) to check for patterns, which might indicate non-linearity, heteroscedasticity, or outliers.\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "\n",
    "#plt.xlim(left=-200,right= 2000)\n",
    "#plt.ylim(bottom=-500, top= 500)\n",
    "\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/ Error Distribution Analysis:\n",
    "#Analyzing the distribution of errors can reveal whether your model consistently underpredicts or overpredicts.\n",
    "# Plotting error distribution\n",
    "plt.hist(residuals, bins=30)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Error Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4/ Identify Outliers and High-Error Instances:\n",
    "# Check for instances with unusually high errors which may be outliers or data quality issues. Understanding these can help in refining the model.\n",
    "# Find rows with the highest residual errors\n",
    "high_error_indices = residuals.nlargest(10).index\n",
    "print(high_error_indices)\n",
    "\n",
    "high_error_rows = df.loc[high_error_indices]\n",
    "print(high_error_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 /Cross-Validation:\n",
    "# Use cross-validation to assess how the model performs across different subsets of data, ensuring that performance isn’t due to random chance.\n",
    "\n",
    "# Compute cross-validation scores\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert negative RMSE to positive RMSE for interpretation\n",
    "rmse_scores = -scores\n",
    "\n",
    "# Print RMSE scores for each fold\n",
    "print(f'Cross-Validation RMSE Scores: {rmse_scores}')\n",
    "\n",
    "# Plot the RMSE scores for each fold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rmse_scores) + 1), rmse_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Cross-Validation RMSE Scores')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(range(1, len(rmse_scores) + 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**! I am not sure if this following error analysis  also works in Xgboost !**\n",
    "Partial Dependence Plots have some limitations, including: \n",
    "\n",
    "1. They only show the relationship between a single feature and the model's predictions, which may not capture complex interactions between features.\n",
    "\n",
    " 2. They require manual sorting or selection of interesting plots, which can be time-consuming and subjective.\n",
    " '''\n",
    "# 6/ Partial dependence plots (PDP)\n",
    "# PDPs help visualize the relationship between the target and specific features, helping you understand the model’s behavior.\n",
    "#from catboost import plot_partial_dependence\n",
    "#column_indices = {col: idx for idx, col in enumerate(df_model.columns)}\n",
    "#print(column_indices)\n",
    "\n",
    "#plot_partial_dependence(model, X, features=[0, 1])  # Replace with relevant feature indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
